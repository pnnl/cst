# Login for mongoku
==**==
worker:worker@tapteal.pnl.gov:27017/?authSource=copper&authMechanism=SCRAM-SHA-1


# PostgreSQL
==**==
https://inqira.io/q/which-database-is-better-for-handling-time-series-data-postgresql-or-mysql
PostgreSQL has some unique features that make it particularly well-suited for
time-series data. For example, it has a specialized data type called timestamp
with time zone that makes it easy to work with data across different time zones.
It also has support for range types and window functions that are useful for
analyzing time-series data. Additionally, PostgreSQL has excellent support for
indexing, which can greatly improve query performance.

## Install yq (https://github.com/mikefarah/yq/#install) to parse the YAML file and retrieve the network name
==**==
NETWORK_NAME=$(yq eval '.networks' postgres-docker-compose.yaml | cut -f 1 -d':')
docker network create $NETWORK_NAME
# or hardcode the network name from the YAML file
    docker network create etl_network

docker-compose --env-file ./.env -f ./postgres-docker-compose.yaml up -d

// Persist in postgress data files
==**==
https://stackoverflow.com/questions/41637505/how-to-persist-data-in-a-dockerized-postgres-database-using-volumes

You can create a common volume for all Postgres data >>
    docker volume create pgdata
or you can set it to the compose file >>

version: "3"
services:
  db:
    image: postgres
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgress
      - POSTGRES_DB=postgres
    ports:
      - "5433:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
    networks:
      - suruse
volumes:
  pgdata:
It will create volume name pgdata and mount this volume to container's path.

You can inspect this volume >>
    docker volume inspect pgdata

Some files need the line endings to be changed (in VScode, in the command
palette select 'Change End of Line Sequence' and change from CRLF to LF).
Windows doesn't like empty lines on shell scripts.
For starting up Docker, running the commands in the shell script in the
terminal works better than running the script itself.

# AIRFLOW
==**==
In the admin interface add a connection the with the password
Either the copy for running the dags one the same server as airflow

# For Jupyter
==**==
# remove old fingerprint if any, not needed in file was empty
ssh-keygen -R boomer
# add the new fingerprint
ssh-keyscan boomer >> ~/.ssh/known_hosts

from the node airflow is running from copy the file to docker
## For UA for ssh remote access to servers
 === from the host, remember passphrase, may be needed when copy to remote host
    ssh-keygen -f ~/copper-key-ecdsa -t ecdsa -b 521
 === user@host is the remote user and host
    ssh-copy-id -i ~/copper-key-ecdsa user@host
need add to cosim-airflow and jupyter through terminal:
    ssh-copy-id -i ~/copper-key-ecdsa d3j331@gage.pnl.gov
 === to run without password/first time may require passphrase

# Info about keygen
==**==
Creating an SSH Key Pair for User Authentication
The simplest way to generate a key pair is to run ssh-keygen without arguments. In this case, it will prompt for the file in which to store keys. Here's an example:

klar (11:39) ~>ssh-keygen Generating public/private rsa key pair.
Enter file in which to save the key (/home/ylo/.ssh/id_rsa):
Enter passphrase (empty for no passphrase):  Enter same passphrase again:
Your identification has been saved in /home/ylo/.ssh/id_rsa.
Your public key has been saved in /home/ylo/.ssh/id_rsa.pub.
The key fingerprint is: SHA256:Up6KjbnEV4Hgfo75YM393QdQsK3Z0aTNBz0DoirrW+c ylo@klar
The key's randomart image is:
+---[RSA 2048]----+
|    .      ..oo..|
|   . . .  . .o.X.|
|    . . o.  ..+ B|
|   .   o.o  .+ ..|
|    ..o.S   o..  |
|   . %o=      .  |
|    @.B...     . |
|   o.=. o. . .  .|
|    .oo  E. . .. |
+----[SHA256]-----+

klar (11:40) ~>
First, the tool asked where to save the file. SSH keys for user authentication are usually stored in the user's .ssh directory under the home directory. However, in enterprise environments, the location is often different. The default key file name depends on the algorithm, in this case id_rsa when using the default RSA algorithm. It could also be, for example, id_dsa or id_ecdsa.

Then it asks to enter a passphrase. The passphrase is used for encrypting the key, so that it cannot be used even if someone obtains the private key file. The passphrase should be cryptographically strong. Our online random password generator is one possible tool for generating strong passphrases.

Choosing an Algorithm and Key Size
SSH supports several public key algorithms for authentication keys. These include:

rsa - an old algorithm based on the difficulty of factoring large numbers. A key size of at least 2048 bits is recommended for RSA; 4096 bits is better. RSA is getting old and significant advances are being made in factoring. Choosing a different algorithm may be advisable. It is quite possible the RSA algorithm will become practically breakable in the foreseeable future. All SSH clients support this algorithm.
dsa - an old US government Digital Signature Algorithm. It is based on the difficulty of computing discrete logarithms. A key size of 1024 would normally be used with it. DSA in its original form is no longer recommended.
ecdsa - a new Digital Signature Algorithm standarized by the US government, using elliptic curves. This is probably a good algorithm for current applications. Only three key sizes are supported: 256, 384, and 521 (sic!) bits. We would recommend always using it with 521 bits, since the keys are still small and probably more secure than the smaller keys (even though they should be safe as well). Most SSH clients now support this algorithm.
ed25519 - this is a new algorithm added in OpenSSH. Support for it in clients is not yet universal. Thus its use in general purpose applications may not yet be advisable.

The algorithm is selected using the -t option and key size using the -b option. The following commands illustrate:

ssh-keygen -t rsa -b 4096
ssh-keygen -t dsa
ssh-keygen -t ecdsa -b 521
ssh-keygen -t ed25519
Specifying the File Name
Normally, the tool prompts for the file in which to store the key. However, it can also be specified on the command line using the -f <filename> option.

ssh-keygen -f ~/tatu-key-ecdsa -t ecdsa -b 521
Copying the Public Key to the Server
To use public key authentication, the public key must be copied to a server and installed in an authorized_keys file. This can be conveniently done using the ssh-copy-id tool. Like this:

ssh-copy-id -i ~/.ssh/tatu-key-ecdsa user@host
Once the public key has been configured on the server, the server will allow any connecting user that has the private key to log in. During the login process, the client proves possession of the private key by digitally signing the key exchange.

WSL3 to change UID
==================
Open regedit, change the parameter
\HKEY_CURRENT_USER\Software\Microsoft\Windows\CurrentVersion\Lxss{cefb...cb50}\DefaultUid
to newuid.

from root
replace uid with newuidpasswd
   sudo nano /etc/passwd
replace uid with newuid group
   sudo nano /etc/group
change the files in th home directory to the named 'user'
   chown -R 'user':'user' /home/'user'


Adding a directories where new modules will be deployed
=======================================================
https://stackoverflow.com/questions/67887138/how-to-install-packages-in-airflow-docker-compose
don't know if the answer is too late, anyway, I've managed to workaround this problem by:

Defining new volume in docker compose, pointing to a directory where new modules will be deployed.
In my case, in Docker compose x-airflow-common: common section, in volumes subsection,
I added: - ${AIRFLOW_PROJ_DIR:-.}/python:/python_extended
Then, in ${AIRFLOW_PROJ_DIR:-.}/python, I can deploy new modules to be used as PYTHONPATH.
Finally, it is possible to define a .env specific per task defined in
Airflow in ${AIRFLOW_PROJ_DIR:-.}/python and load a per-task specific .env files in the dag.py of the task.

== So, my docker-compose.yml looks like:

version: '3.8'
x-airflow-common:
  &airflow-common
  # In order to add custom dependencies or upgrade provider packages you can use your extended image.
  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml
  # and uncomment the "build" line below, Then run `docker-compose build` to build the images.

  #image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.7.0}
  build: .

  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    # For backward compatibility, with Airflow <2.3
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    # yamllint disable rule:line-length
    # Use simple http server on scheduler for health checks
    # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server
    # yamllint enable rule:line-length
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks
    # for other purpose (development, test and especially production usage) build/extend Airflow image.
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    PYTHONPATH: '$PYTHONPATH;/python_extended'
  volumes:
    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
    - ${AIRFLOW_PROJ_DIR:-.}/python:/python_extended
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    &airflow-common-depends-on
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy



== And my DAGs start with:
import sys, os, logging
sys.path.append(os.path.abspath("/python_extended"))
logging.info(os.environ['PYTHONPATH'])

...

from dotenv import load_dotenv

path = '/python_extended/whatever.env'
print(f'> Using .env file: {path}')
load_dotenv(path)

This way, keeping the current docker deploy of the image,
you can add new modules by just deploying them to your local path in ${AIRFLOW_PROJ_DIR:-.}/python.